{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langcore-chains (from versions: none)\n",
      "ERROR: No matching distribution found for langcore-chains\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet neo4j langchain-neo4j langchain-community langchain-core langchain-experimental langchain-openai json-repair langcore-chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-community langchain-core \"numpy<2.0\" langchain langchain-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip --quiet install langchain-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0 in c:\\users\\abhis\\miniconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Neo4j environment as the graph store -- comes with visualizations used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from langchain.graphs.neo4j_graph import Neo4jGraph \n",
    "\n",
    "load_dotenv()\n",
    "neo_pass = os.getenv(\"NEO4J_PASS\")\n",
    "neo_db_id = os.getenv(\"DB_ID\")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"neo4j+s://f5c81351.databases.neo4j.io\",\n",
    "    username=\"neo4j\",\n",
    "    password=neo_pass,\n",
    "    enhanced_schema=True\n",
    "    # refresh_schema=Fa lse\n",
    ")\n",
    "\n",
    "def clean_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    DETACH DELETE n\n",
    "    \"\"\"\n",
    "    graph.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GraphQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import GraphQAChain old\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean graph if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to model (OpenAI for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[api_key] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain LLM Graph Transformer\n",
    "Establish connection with GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "graph_transformer_prompt = \"\"\"\n",
    "When creating nodes, capitalize the id.\n",
    "For instance Apple becomes APPLE.\n",
    "\"\"\"\n",
    "llm_transformer = LLMGraphTransformer(llm=llm, additional_instructions=graph_transformer_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_19904\\899858828.py:10: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(test_prompt.format(content=test_text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nodes:\\n- APPLE\\n- BANANA\\n\\nEdges:\\n- Mentioning (APPLE, BANANA)' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 41, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-eced44c0-1404-47e6-a9eb-fb5d3bf7fb82-0' usage_metadata={'input_tokens': 41, 'output_tokens': 23, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "Create a graph representation of the following content:\n",
    "{content}\n",
    "\n",
    "When creating nodes, capitalize the id.\n",
    "For instance Apple becomes APPLE.\n",
    "\"\"\"\n",
    "\n",
    "test_text = \"This is a test document mentioning Apple and Banana.\"\n",
    "# response = llm(test_prompt.format(content=test_text))\n",
    "response = llm.invoke(test_prompt.format(content=test_text))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='1', type='Document', properties={})], relationships=[], source=Document(metadata={}, page_content='Apples are bright red fruits.\\nBananas in this context are a code word for a yellow car.'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "with open('apple_document.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "doc = [Document(page_content=text)]\n",
    "graph_doc = await llm_transformer.aconvert_to_graph_documents(doc)\n",
    "print(graph_doc)\n",
    "# graph.add_graph_documents(graph_doc, include_source=True, baseEntityLabel=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Bananas', type='Fruit', properties={}), Node(id='Apples', type='Codeword', properties={}), Node(id='Red Trucks', type='Vehicle', properties={})], relationships=[Relationship(source=Node(id='Apples', type='Codeword', properties={}), target=Node(id='Red Trucks', type='Vehicle', properties={}), type='REPRESENTS', properties={})], source=Document(metadata={'id': '83b84d36a24f3f060e3107fa7bc0d748'}, page_content='Bananas are bright yellow fruits.\\nApples in this context are a code word for Red trucks.'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "with open('banana_document.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "doc = [Document(page_content=text)]\n",
    "graph_doc = await llm_transformer.aconvert_to_graph_documents(doc)\n",
    "graph.add_graph_documents(graph_doc, include_source=True, baseEntityLabel=True)\n",
    "print(graph_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Neuroscience paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-U4CXypb0uMdI23Afx5jnxLiJ on tokens per min (TPM): Limit 40000, Requested 49515. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m documents \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mtext)]\n\u001b[1;32m----> 6\u001b[0m graph_documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_transformer\u001b[38;5;241m.\u001b[39maconvert_to_graph_documents(documents)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# baseEntityLabel allows us to optimize data retrieval even though we don't\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# know all node labels and don't keep track of indices\u001b[39;00m\n\u001b[0;32m     10\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_graph_documents(graph_documents, include_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, baseEntityLabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_experimental\\graph_transformers\\llm.py:1031\u001b[0m, in \u001b[0;36mLLMGraphTransformer.aconvert_to_graph_documents\u001b[1;34m(self, documents, config)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03mAsynchronously convert a sequence of documents into graph documents.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1028\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprocess_response(document, config))\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents\n\u001b[0;32m   1030\u001b[0m ]\n\u001b[1;32m-> 1031\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_experimental\\graph_transformers\\llm.py:942\u001b[0m, in \u001b[0;36mLLMGraphTransformer.aprocess_response\u001b[1;34m(self, document, config)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03mAsynchronously processes a single document, transforming it into a\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03mgraph document.\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    941\u001b[0m text \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mpage_content\n\u001b[1;32m--> 942\u001b[0m raw_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_call:\n\u001b[0;32m    944\u001b[0m     raw_schema \u001b[38;5;241m=\u001b[39m cast(Dict[Any, Any], raw_schema)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3064\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3062\u001b[0m     part \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(step\u001b[38;5;241m.\u001b[39mainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[1;32m-> 3064\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   3065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3066\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3771\u001b[0m, in \u001b[0;36mRunnableParallel.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3769\u001b[0m     \u001b[38;5;66;03m# copy to avoid issues from the caller mutating the steps during invoke()\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m     steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps__)\n\u001b[1;32m-> 3771\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m   3772\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[0;32m   3773\u001b[0m             _ainvoke_step(\n\u001b[0;32m   3774\u001b[0m                 step,\n\u001b[0;32m   3775\u001b[0m                 \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3776\u001b[0m                 \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   3777\u001b[0m                 config,\n\u001b[0;32m   3778\u001b[0m                 key,\n\u001b[0;32m   3779\u001b[0m             )\n\u001b[0;32m   3780\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3781\u001b[0m         )\n\u001b[0;32m   3782\u001b[0m     )\n\u001b[0;32m   3783\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(steps, results))\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3761\u001b[0m, in \u001b[0;36mRunnableParallel.ainvoke.<locals>._ainvoke_step\u001b[1;34m(step, input, config, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[1;32m-> 3761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m         step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, child_config), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[0;32m   3763\u001b[0m     )\n\u001b[0;32m   3764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, child_config))\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5364\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5358\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m   5359\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5360\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5361\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5362\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5363\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[0;32m   5365\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5366\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5367\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5368\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    306\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 307\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[0;32m    308\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    309\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    310\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    311\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    312\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    313\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    314\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    790\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    795\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[0;32m    797\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    798\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    746\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    754\u001b[0m             ]\n\u001b[0;32m    755\u001b[0m         )\n\u001b[1;32m--> 756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    757\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    758\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    760\u001b[0m ]\n\u001b[0;32m    761\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:924\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 924\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[0;32m    925\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:825\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result, response, generation_info\n\u001b[0;32m    828\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1661\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1659\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m   1660\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1663\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1664\u001b[0m             {\n\u001b[0;32m   1665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   1677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   1680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1694\u001b[0m             },\n\u001b[0;32m   1695\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   1696\u001b[0m         ),\n\u001b[0;32m   1697\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1698\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1699\u001b[0m         ),\n\u001b[0;32m   1700\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1701\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1702\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   1703\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1843\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1830\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1831\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1839\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1840\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1841\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1842\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1535\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1538\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1539\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1540\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1541\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1542\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1543\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1624\u001b[0m         input_options,\n\u001b[0;32m   1625\u001b[0m         cast_to,\n\u001b[0;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1624\u001b[0m         input_options,\n\u001b[0;32m   1625\u001b[0m         cast_to,\n\u001b[0;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhis\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1638\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m   1637\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1641\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1642\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1646\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1647\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-3.5-turbo in organization org-U4CXypb0uMdI23Afx5jnxLiJ on tokens per min (TPM): Limit 40000, Requested 49515. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "with open('verylarge_document.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "documents = [Document(page_content=text)]\n",
    "graph_documents = await llm_transformer.aconvert_to_graph_documents(documents)\n",
    "\n",
    "# baseEntityLabel allows us to optimize data retrieval even though we don't\n",
    "# know all node labels and don't keep track of indices\n",
    "graph.add_graph_documents(graph_documents, include_source=True, baseEntityLabel=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # assign a unique document ID\n",
    "# document_id = 'apple2'\n",
    "# for graph_document in graph_documents:\n",
    "#     # For nodes\n",
    "#     for node in graph_document.nodes:\n",
    "#         # Prepare Cypher query to add document_id to the node\n",
    "#         query = \"\"\"\n",
    "#         MATCH (n)\n",
    "#         SET n.document_id = 'apple2'\n",
    "#         RETURN n\n",
    "#         \"\"\"\n",
    "#         # Use graph.query() to run the query\n",
    "#         graph.query(query, {\"node_id\": node.id, \"document_id\": document_id})\n",
    "\n",
    "\n",
    "\n",
    "# no_schema = LLMGraphTransformer(llm=llm)\n",
    "# data = await no_schema.aconvert_to_graph_documents(documents)\n",
    "# graph.add_graph_documents(data)\n",
    "\n",
    "# print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install  --quiet --upgrade langchain neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph.refresh_schema()\n",
    "# CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "# Instructions:\n",
    "# Use only the provided relationship types and properties in the schema.\n",
    "# Do not use any other relationship types or properties that are not provided.\n",
    "# If no exact match is found for the input, generate a Cypher query using a fuzzy matching operator like `CONTAINS` to find related nodes.\n",
    "# Schema:\n",
    "# {schema}\n",
    "# Note: Do not include any explanations or apologies in your responses.\n",
    "# Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "# Do not include any text except the generated Cypher statement.\n",
    "# Return every node as whole, do not return only the properties.\n",
    "\n",
    "# The question is:\n",
    "# {question}\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task:Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not assume any specific relationship types unless explicitly needed.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "When querying, look for nodes connected to the target node and their relationships. Do not care about the label of the nodes.\n",
    "Also when querying, do not care about the direction of the relationship, so use `-` rather than `->`.\n",
    "When comparing to the target node, use a fuzzy matching operator like `CONTAINS` rather than strict .id: to find related nodes.\n",
    "Also, capitalize the target node appropriately to get a match in the graph.\n",
    "When querying, also match for a node x that is the document that connects to relevant nodes, where x.id is equal to the inputted document_id. However, do not return this node x in the context.\n",
    "\n",
    "\n",
    "Example 1 (specific to a particular schema):\n",
    "Question: What are the proteins associated with 'tir840'?\n",
    "Generated Cypher: \n",
    "MATCH (x)-[]-(p)-[r]-(c)\n",
    "WHERE c.id CONTAINS 'Tir840' AND NOT p:Document AND NOT c:Document AND x.id=\"{document_id}\"\n",
    "RETURN p, r, c\n",
    "\n",
    "Example 2 (specific to a particular schema):\n",
    "Question: Give me an overview of the Concept 'Tir840 Site'.\n",
    "Generated Cypher:\n",
    "MATCH (x)-[]-(p)-[r]-(c)\n",
    "WHERE c.id CONTAINS 'Tir840' AND NOT p:Document AND NOT c:Document AND x.id=\"{document_id}\" \n",
    "RETURN p, r, c\n",
    "\n",
    "Example 3 (specific to a particular schema):\n",
    "Question: What is Glur1?\n",
    "Generated Cypher: \n",
    "MATCH (x)-[]-(p)-[r]-(c)\n",
    "WHERE c.id CONTAINS 'Glur1' AND NOT p:Document AND NOT c:Document AND x.id=\"{document_id}\"\n",
    "RETURN p, r, c\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "Note: Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "Return every node as whole, do not return only the properties.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\", \"document_id\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm, \n",
    "    graph=graph, \n",
    "    verbose=True, \n",
    "    validate_cypher=True, # delete later? for correcting relationship directions\n",
    "    allow_dangerous_requests=True, \n",
    "    return_intermediate_steps=True,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (x)-[]-(p)-[r]-(c)\n",
      "WHERE c.id CONTAINS 'Calcineurin' AND NOT p:Document AND NOT c:Document AND x.id=\"91aa6e986408d31253a5d2706d507ad8\"\n",
      "RETURN p, r, c\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# user_query = \"Give me an overviwe of Glur1\"\n",
    "# user_query = \"What other molecules affects Pka and what are the relationships?\"\n",
    "user_query = \"What is Calcineurin\"\n",
    "# user_query = user_query.lower().strip()\n",
    "graph.refresh_schema()\n",
    "context = chain.invoke({\"query\": user_query, \"document_id\": \"91aa6e986408d31253a5d2706d507ad8\"}) \n",
    "\n",
    "# chain.invoke(\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "# print(context)    \n",
    "print(context[\"result\"])\n",
    "# print(context[\"intermediate_steps\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:[Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), Node(id='Synapses', type='Concept', properties={}), Node(id='Cell Signalling', type='Concept', properties={}), Node(id='Ca2+ Transients', type='Concept', properties={}), Node(id='Ltp', type='Concept', properties={}), Node(id='Memory Storing', type='Concept', properties={}), Node(id='Pka', type='Protein', properties={}), Node(id='Glur1', type='Protein', properties={}), Node(id='Ampar', type='Protein', properties={}), Node(id='Pkc', type='Protein', properties={}), Node(id='Tir840 Site', type='Concept', properties={}), Node(id='Protein Phosphatase 1', type='Protein', properties={}), Node(id='Ampa-Receptors', type='Protein', properties={}), Node(id='Nmda-Receptors', type='Protein', properties={}), Node(id='Dendritic Spine Formation', type='Concept', properties={}), Node(id='Pp1', type='Protein', properties={}), Node(id='Camp', type='Concept', properties={}), Node(id='Ca2+-Dependent Isoforms', type='Concept', properties={}), Node(id='Ca2+-Independent Isoforms', type='Concept', properties={}), Node(id='Pp2B', type='Protein', properties={}), Node(id='Calcineurin', type='Concept', properties={}), Node(id='Second Messenger', type='Concept', properties={}), Node(id='Calmodulin', type='Concept', properties={}), Node(id='B Regulatory Subunit', type='Concept', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), target=Node(id='Synapses', type='Concept', properties={}), type='ABUNDANT_IN', properties={}), Relationship(source=Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), target=Node(id='Cell Signalling', type='Concept', properties={}), type='MEDIATES', properties={}), Relationship(source=Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), target=Node(id='Ca2+ Transients', type='Concept', properties={}), type='RESPONDS_TO', properties={}), Relationship(source=Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), target=Node(id='Ltp', type='Concept', properties={}), type='INVOLVED_IN', properties={}), Relationship(source=Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), target=Node(id='Memory Storing', type='Concept', properties={}), type='IMPLICATED_IN', properties={}), Relationship(source=Node(id='Ca2+/Calmodulin-Dependent Protein Kinase Ii', type='Protein', properties={}), target=Node(id='Camkii Phosphorylated State', type='Concept', properties={}), type='PHOSPHORYLATES', properties={}), Relationship(source=Node(id='Pka', type='Protein', properties={}), target=Node(id='Ltp', type='Concept', properties={}), type='MODULATES', properties={}), Relationship(source=Node(id='Pka', type='Protein', properties={}), target=Node(id='Glur1', type='Protein', properties={}), type='PHOSPHORYLATES', properties={}), Relationship(source=Node(id='Pkc', type='Protein', properties={}), target=Node(id='Glur1', type='Protein', properties={}), type='PHOSPHORYLATES', properties={}), Relationship(source=Node(id='Pkc', type='Protein', properties={}), target=Node(id='Tir840 Site', type='Concept', properties={}), type='PHOSPHORYLATES', properties={}), Relationship(source=Node(id='Protein Phosphatase 1', type='Protein', properties={}), target=Node(id='Ampa-Receptors', type='Protein', properties={}), type='REGULATES', properties={}), Relationship(source=Node(id='Protein Phosphatase 1', type='Protein', properties={}), target=Node(id='Nmda-Receptors', type='Protein', properties={}), type='REGULATES', properties={}), Relationship(source=Node(id='Protein Phosphatase 1', type='Protein', properties={}), target=Node(id='Dendritic Spine Formation', type='Concept', properties={}), type='MODULATES', properties={}), Relationship(source=Node(id='Protein Phosphatase 1', type='Protein', properties={}), target=Node(id='Camp', type='Concept', properties={}), type='MODULATES', properties={}), Relationship(source=Node(id='Protein Phosphatase 1', type='Protein', properties={}), target=Node(id='Ca2+', type='Concept', properties={}), type='MODULATES', properties={}), Relationship(source=Node(id='Pp2B', type='Protein', properties={}), target=Node(id='Calcineurin', type='Concept', properties={}), type='ALSO_KNOWN_AS', properties={}), Relationship(source=Node(id='Pp2B', type='Protein', properties={}), target=Node(id='Second Messenger', type='Concept', properties={}), type='MODULATED_BY', properties={}), Relationship(source=Node(id='Pp2B', type='Protein', properties={}), target=Node(id='Calmodulin', type='Concept', properties={}), type='MODULATED_BY', properties={}), Relationship(source=Node(id='Pp2B', type='Protein', properties={}), target=Node(id='B Regulatory Subunit', type='Concept', properties={}), type='MODULATED_BY', properties={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Graph with Neo4j browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(f'https://{neo_db_id}.databases.neo4j.io/browser/', new=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Neo4j browser, use\n",
    "MATCH(n) return n\n",
    "to display graph (Cypher query language)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
