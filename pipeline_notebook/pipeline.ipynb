{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain-neo4j langchain_ollama pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "neo_pass = os.getenv(\"NEO4J_PASS\")\n",
    "\n",
    "url = \"neo4j+s://f5c81351.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = neo_pass\n",
    "graph = Neo4jGraph(url=url, username=username, password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF Documents\n",
    "split text into chunks and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from collections import Counter\n",
    "import re #Regex\n",
    "\n",
    "def _extract_keywords(text: str, top_n: int = 5) -> List[str]:\n",
    "\n",
    "    words = re.findall(r\"r\\w+\", text.lower())\n",
    "\n",
    "    stop_words = set(\n",
    "        [\n",
    "            \"the\",\n",
    "            \"a\",\n",
    "            \"an\",\n",
    "            \"and\",\n",
    "            \"or\",\n",
    "            \"but\",\n",
    "            \"in\",\n",
    "            \"on\",\n",
    "            \"at\",\n",
    "            \"to\",\n",
    "            \"for\",\n",
    "            \"of\",\n",
    "            \"with\",\n",
    "            \"by\",\n",
    "        ]\n",
    "    )\n",
    "    filtered_words = [\n",
    "        word for word in words if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "\n",
    "    return [word for word, count in Counter(filtered_words).most_common(top_n)]\n",
    "\n",
    "def load_and_process_pdf(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict]:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "    pages = loader.load() # load pages\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "\n",
    "    splits = text_splitter.split_documents(pages) # split the pages using LangChain's text_splitter\n",
    "\n",
    "    processed_chunks = []\n",
    "    for i, chunk in enumerate(splits):\n",
    "       metadata = {\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": pdf_path,\n",
    "            \"page_number\": chunk.metadata.get(\"page\", None),\n",
    "            \"total_length\": len(chunk.page_content),\n",
    "            \"keywords\": _extract_keywords(chunk.page_content),\n",
    "            \"text_preview\": (\n",
    "                chunk.page_content[:100] + \"...\"\n",
    "                if len(chunk.page_content) > 100\n",
    "                else chunk.page_content\n",
    "            ),\n",
    "        }\n",
    "       processed_chunks.append({\"text\": chunk.page_content, \"metadata\": metadata})\n",
    "    return processed_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 7\n",
      "\n",
      "Chunk 0:\n",
      "Text Preview: Abi Kakolla \n",
      "Toronto, ON | (647) 957-7403 | kakolla@usc.edu| linkedin.com/in/kakolla|kakolla.com \n",
      " \n",
      "...\n",
      "Keywords: ['rvard', 'ring', 'rning', 'ral', 'ronto']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 1:\n",
      "Text Preview: ● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural ...\n",
      "Keywords: ['ral', 'ration', 'rate', 'rid', 'rating']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 2:\n",
      "Text Preview: Lead Software Developer                  Nov 2021 - June 2022 \n",
      "inLoop (sponsored by Deloitte)       ...\n",
      "Keywords: ['ronto', 'red', 'ript', 'rebase', 'riving']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 3:\n",
      "Text Preview: PROJECTS                                                                                            ...\n",
      "Keywords: ['rvard', 'rraform', 'ricks', 'raspberry', 'rest']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 4:\n",
      "Text Preview: ● Implemented a Random Forest Classifier model in Databricks using Google Kubernetes Engine resource...\n",
      "Keywords: ['rithm', 'react', 'rest', 'redictions', 'real']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 5:\n",
      "Text Preview: Scribo – AI Email classifier app   github.com/kakolla/Scribo \n",
      "Technologies: Python, Tkinter, Gmail A...\n",
      "Keywords: ['ribo', 'rvised', 'read', 'ript', 'rning']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 6:\n",
      "Text Preview: Tools: Git, Docker, Kubernetes, VS Code, Eclipse, Linux/Unix (Debian) \n",
      "Technologies: Google Gemini, ...\n",
      "Keywords: ['rnetes', 'ral', 'rduino', 'rks']\n",
      "Page Number: 0\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"resume.pdf\"\n",
    "\n",
    "chunks = load_and_process_pdf(pdf_path)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Text Preview: {chunk['metadata']['text_preview']}\")\n",
    "    print(f\"Keywords: {chunk['metadata']['keywords']}\")\n",
    "    print(f\"Page Number: {chunk['metadata']['page_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data chunks to Neo4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_chunks(chunks: List[Dict]):\n",
    "    # graph.query(\"MATCH (n) DETACH DELETE n\") # cleans graph\n",
    "\n",
    "    # cypher query to create the chunks & their attributes\n",
    "    create_chunk_query = \"\"\"\n",
    "    MERGE (chunk:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET\n",
    "        chunk.source = $source,\n",
    "        chunk.page_number = $page_number,\n",
    "        chunk.total_length = $total_length,\n",
    "        chunk.text_preview = $text_preview,\n",
    "        chunk.full_text = $full_text\n",
    "        WITH chunk\n",
    "        UNWIND $keywords AS keyword\n",
    "        MERGE (kw:Keyword {name: keyword})\n",
    "        MERGE (chunk)-[:HAS_KEYWORD]->(kw)\n",
    "        RETURN chunk\n",
    "    \"\"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        graph.query(\n",
    "            create_chunk_query,\n",
    "            params={\n",
    "                \"chunk_id\": chunk[\"metadata\"][\"chunk_id\"],\n",
    "                \"source\": chunk[\"metadata\"][\"source\"],\n",
    "                \"page_number\": chunk[\"metadata\"][\"page_number\"],\n",
    "                \"total_length\": chunk[\"metadata\"][\"total_length\"],\n",
    "                \"text_preview\": chunk[\"metadata\"][\"text_preview\"],\n",
    "                \"full_text\": chunk[\"text\"],\n",
    "                \"keywords\": chunk[\"metadata\"][\"keywords\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "create_graph_from_chunks(chunks[:200])\n",
    "\n",
    "# after storing the data, create a unique constraint to make sure data is secure\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunk_id IS UNIQUE\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "embedding_dim = 3072\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector index for similarity search using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.index.vector.createNodeIndex' has been replaced by 'CREATE VECTOR INDEX')} {position: line: 2, column: 13, offset: 13} for query: \"\\n            CALL db.index.vector.createNodeIndex(\\n                'chunk_vector_index',\\n                'Chunk',\\n                'embedding',\\n                $dim,\\n                'cosine'\\n            )\\n            \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7/7 chunks\n"
     ]
    }
   ],
   "source": [
    "def generate_embedding(text: str) -> List[float]:\n",
    "   \n",
    "    try:\n",
    "        embedding = embeddings.embed_query(text)\n",
    "\n",
    "        embedding = [float(x) for x in embedding]\n",
    "\n",
    "        magnitude = sum(x * x for x in embedding) ** 0.5\n",
    "        if magnitude > 0:\n",
    "            embedding = [x / magnitude for x in embedding]\n",
    "\n",
    "        if len(embedding) != embedding_dim:\n",
    "            if len(embedding) < embedding_dim:\n",
    "                embedding.extend([0.0] * (embedding_dim - len(embedding)))\n",
    "            else:\n",
    "                embedding = embedding[:embedding_dim]\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return [0.0] * embedding_dim\n",
    "\n",
    "\n",
    "# we create the vector index using the above function for generating embeddings\n",
    "def create_vector_index(chunks: List[Dict]):\n",
    "\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            DROP INDEX chunk_vector_index IF EXISTS \n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.createNodeIndex(\n",
    "                'chunk_vector_index',\n",
    "                'Chunk',\n",
    "                'embedding',\n",
    "                $dim,\n",
    "                'cosine'\n",
    "            )\n",
    "            \"\"\",\n",
    "            params={\"dim\": embedding_dim},\n",
    "        )\n",
    "\n",
    "        batch_size = 10\n",
    "        total_processed = 0\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i : i + batch_size]\n",
    "            batch_embeddings = []\n",
    "\n",
    "            for chunk in batch:\n",
    "                embedding = generate_embedding(chunk[\"text\"])\n",
    "                batch_embeddings.append(\n",
    "                    {\"chunk_id\": chunk[\"metadata\"][\"chunk_id\"], \"embedding\": embedding}\n",
    "                )\n",
    "\n",
    "            batch_update_query = \"\"\"\n",
    "            UNWIND $batch AS item\n",
    "            MATCH (chunk:Chunk {chunk_id: item.chunk_id})\n",
    "            SET chunk.embedding = item.embedding\n",
    "            \"\"\"\n",
    "\n",
    "            graph.query(batch_update_query, params={\"batch\": batch_embeddings})\n",
    "\n",
    "            total_processed += len(batch)\n",
    "            print(f\"Processed {total_processed}/{len(chunks)} chunks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector index: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "try:\n",
    "    create_vector_index(chunks[:200])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create vector index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform vector search on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'chunk_vector_index', 'type': 'VECTOR', 'labelsOrTypes': ['Chunk'], 'properties': ['embedding'], 'options': {'indexProvider': 'vector-2.0', 'indexConfig': {'vector.hnsw.m': 16, 'vector.hnsw.ef_construction': 100, 'vector.dimensions': 3072, 'vector.similarity_function': 'COSINE', 'vector.quantization.enabled': True}}}]\n"
     ]
    }
   ],
   "source": [
    "def verify_vector_index():\n",
    "    query = \"\"\"\n",
    "    SHOW INDEXES\n",
    "    YIELD name, type, labelsOrTypes, properties, options\n",
    "    WHERE name = 'chunk_vector_index'\n",
    "    \"\"\"\n",
    "    return graph.query(query)\n",
    "\n",
    "\n",
    "def vector_search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "   \n",
    "    try:\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "        search_query = \"\"\"\n",
    "        MATCH (c:Chunk)\n",
    "        WITH c, vector.similarity.cosine(c.embedding, $embedding) AS score\n",
    "        WHERE score > 0.7\n",
    "        RETURN \n",
    "            c.chunk_id AS chunk_id,\n",
    "            c.source AS source,\n",
    "            c.page_number AS page_number,\n",
    "            c.text_preview AS text_preview,\n",
    "            c.full_text AS full_text,\n",
    "            c.total_length AS total_length,\n",
    "            score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT $limit\n",
    "        \"\"\"\n",
    "\n",
    "        results = graph.query(\n",
    "            search_query, params={\"embedding\": query_embedding, \"limit\": top_k}\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Vector search error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(verify_vector_index())\n",
    "for x in vector_search(\"What is a biofuel?\"):\n",
    "    print(x)\n",
    "    print(x['chunk_id'])\n",
    "    print(x['source'])\n",
    "    print(x['page_number'])\n",
    "    print(x['text_preview'])\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know, there is no mention of a person's name in the provided text. \\n\\nThanks for asking!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "\n",
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,  \n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='chunk_vector_index',  \n",
    "    node_label='Chunk',  \n",
    "    text_node_properties=['full_text'], \n",
    "    embedding_node_property='embedding'\n",
    ")\n",
    "\n",
    "retriever = neo4j_vector_store.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize the Ollama model\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is the person's name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
