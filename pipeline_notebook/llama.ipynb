{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from langchain.graphs.neo4j_graph import Neo4jGraph \n",
    "# from langchain.chains import GraphQAChain old\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "\n",
    "load_dotenv()\n",
    "neo_pass = os.getenv(\"NEO4J_PASS\")\n",
    "neo_db_id = os.getenv(\"DB_ID\")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"neo4j+s://f5c81351.databases.neo4j.io\",\n",
    "    username=\"neo4j\",\n",
    "    password=neo_pass,\n",
    "    enhanced_schema=True\n",
    "    # refresh_schema=Fa lse\n",
    ")\n",
    "\n",
    "def clean_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (n)\n",
    "    DETACH DELETE n\n",
    "    \"\"\"\n",
    "    graph.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_ollama import ChatOllama  # using chatOllama\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)  # You can change the model to 'llama3' or others\n",
    "llm_transformer_filtered = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# additional_instructions = \"\"\"\n",
    "# When creating entities, add a \"document_id\" property to each node and set it to the document's unique ID.\n",
    "# For example, if the document ID is \"doc123\", each created node should include `document_id: \"doc123\"`.\n",
    "# Query example: \n",
    "# CREATE (n:NodeLabel) \n",
    "# SET n.document_id = \"doc123\" \n",
    "# RETURN n\n",
    "# \"\"\"\n",
    "\n",
    "# Use LLMGraphTransformer with Ollama\n",
    "# llm_transformer = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     additional_instructions=additional_instructions,\n",
    "#     ignore_tool_usage=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 chunks processed\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_path = \"resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "pages = loader.load() # load pages\n",
    "\n",
    "# chunk overlap is the shared context window between chunks--allows context to be maintained across chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
    "\n",
    "splits = text_splitter.split_documents(pages) # split the pages using LangChain's text_splitter\n",
    "\n",
    "processed_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(splits):\n",
    "    # Process the chunk\n",
    "    metadata = {\n",
    "        \"chunk_id\": i,\n",
    "        \"source\": pdf_path,\n",
    "        \"page_number\": chunk.metadata.get(\"page\", None),\n",
    "        \"total_length\": len(chunk.page_content),\n",
    "        \"text_preview\": (\n",
    "            chunk.page_content[:100] + \"...\"\n",
    "            if len(chunk.page_content) > 100\n",
    "                else chunk.page_content\n",
    "        ),\n",
    "\n",
    "\n",
    "    }\n",
    "    # Store the metadata for each chunk after processing\n",
    "    processed_chunks.append({\"text\": chunk.page_content, \"metadata\": metadata})\n",
    "\n",
    "print(str(len(processed_chunks)) + \" chunks processed\")\n",
    "# print(processed_chunks[6]['metadata']['text_preview'])\n",
    "# print(processed_chunks[6]['metadata'])\n",
    "# print(processed_chunks[0]['text_preview'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "# Create graph using the processed chunks\n",
    "def create_graph(chunks: List[Dict]):\n",
    "    # cypher query to create the chunks & their attributes\n",
    "    create_chunk_query = \"\"\"\n",
    "    MERGE (chunk:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET\n",
    "        chunk.source = $source,\n",
    "        chunk.page_number = $page_number,\n",
    "        chunk.total_length = $total_length,\n",
    "        chunk.text_preview = $text_preview,\n",
    "        chunk.full_text = $full_text\n",
    "        RETURN chunk\n",
    "    \"\"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        graph.query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Abi Kakolla', type='Person', properties={}), Node(id='University Of Southern California', type='Organization', properties={}), Node(id='Los Angeles, Ca', type='Location', properties={}), Node(id='Usc Center For Neural Engineering', type='Organization', properties={})], relationships=[], source=Document(metadata={'chunk_id': 0, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 949, 'text_preview': 'Abi Kakolla \\nToronto, ON | (647) 957-7403 | kakolla@usc.edu| linkedin.com/in/kakolla|kakolla.com \\n \\n...'}, page_content='Abi Kakolla \\nToronto, ON | (647) 957-7403 | kakolla@usc.edu| linkedin.com/in/kakolla|kakolla.com \\n \\nEDUCATION \\nUniversity of Southern California                                                                                                                     Los Angeles, CA \\nBachelor of Science in Computer Science (GPA: 3.71)                  Dec 2026 \\nCoursework: Data Structures & Algorithms (C++), Embedded Systems (C), Solidworks  \\nHackathons: HackHarvard (Harvard), LAHacks (UCLA), HackSC (USC), CalHacks (Berkeley)  \\nInterests: Software Engineering, Machine Learning, Neuroscience \\n \\nEXPERIENCE \\nMachine Learning Researcher                        Aug 2023 - Present \\nUSC Center for Neural Engineering                                                                                          Los Angeles, CA \\n● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural network using MeshLab and Python')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'chunk_id': 1, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 926, 'text_preview': '● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural ...'}, page_content='● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural network using MeshLab and Python  \\n● Developed open-source software to grid a dataset of 6 million neural points that enhanced the neural mesh model resolution by 348%, \\nenabling other teams to create more accurate models and save on computational costs  \\n● Reduced neural network generation by 75% by integrating unsupervised machine learning algorithms, enabling faster simulations  \\n● Currently developing a Hybrid Retrieval-Augmented Generation (RAG) pipeline for research faculty integrating Graph retrieval agents \\nwith Neo4j and LangChain, and vector-based agents with Pinecone, to enable neuroscience-focused text and citation generation \\nLead Software Developer                  Nov 2021 - June 2022 \\ninLoop (sponsored by Deloitte)                                                                           Toronto, ON')), GraphDocument(nodes=[Node(id='Lead Software Developer', properties={}), Node(id='Inloop', properties={}), Node(id='Toronto, On', properties={}), Node(id='Javascript', properties={}), Node(id='Html', properties={}), Node(id='Css', properties={}), Node(id='Firebase', properties={}), Node(id='Figma', properties={})], relationships=[], source=Document(metadata={'chunk_id': 2, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 729, 'text_preview': 'Lead Software Developer                  Nov 2021 - June 2022 \\ninLoop (sponsored by Deloitte)       ...'}, page_content='Lead Software Developer                  Nov 2021 - June 2022 \\ninLoop (sponsored by Deloitte)                                                                           Toronto, ON \\n● Led a team of 3 developers to design and build a website using JavaScript, HTML, and CSS, that “gamified” news consumption am ong \\nteens in Toronto \\n● Implemented a Firebase database for website analytics, driving data-driven decision-making that increased user engagement by 200% and \\nattracted 400 weekly visitors within two weeks \\n● Enhanced UI/UX improvements using Figma while working with the marketing department, significantly enhancing visual appeal, a nd \\nsuccessfully pitched the product to sponsors while securing support  \\n \\nPROJECTS')), GraphDocument(nodes=[Node(id='Clean Sweep', type='Project', properties={}), Node(id='Harvard Hackathon', type='Event', properties={}), Node(id='Opencv', type='Technology', properties={}), Node(id='React', type='Technology', properties={}), Node(id='Terraform', type='Technology', properties={}), Node(id='Databricks', type='Technology', properties={}), Node(id='Raspberry Pi', type='Technology', properties={}), Node(id='Scikit-Learn', type='Technology', properties={}), Node(id='Google Distance Matrix Api', type='Technology', properties={})], relationships=[], source=Document(metadata={'chunk_id': 3, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 927, 'text_preview': 'PROJECTS                                                                                            ...'}, page_content='PROJECTS                                                                                                                                                              \\nClean Sweep – Harvard Hackathon Winner 2024                                                           devpost.com/software/cleansweep-tjq36w \\nTechnologies: OpenCV, React, Terraform, Databricks, Raspberry Pi, Scikit -learn, Google Distance Matrix API \\n● Placed as one of the winners out of 140 teams across North America at Hack Harvard for best use of Terraform  \\n● Developed a smart city waste management platform with a team of 4 that optimized collection routes for sanitation trucks  \\n● Wrote the image contour algorithm using OpenCV to detect real -time trash levels and sent data via a REST API to a Raspberry Pi server  \\n● Implemented a Random Forest Classifier model in Databricks using Google Kubernetes Engine resources to make predictions using  the')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'chunk_id': 4, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 941, 'text_preview': '● Implemented a Random Forest Classifier model in Databricks using Google Kubernetes Engine resource...'}, page_content='● Implemented a Random Forest Classifier model in Databricks using Google Kubernetes Engine resources to make predictions using  the \\nreal-time data. Integrated these predictions into a React frontend, displaying optimal routes to the user (truck drivers)  \\nGenetic Optimization Algorithm    github.com/kakolla/genetic-algorithm \\nTechnologies: C++, React, TypeScript, TailwindCSS, REST API \\n● Designed a heuristic genetic optimization algorithm in C++ that uses Uniform Chromosome Crossover to develop higher -fitness \\nsolutions for target images and text \\n● Built an interactive frontend with React and TypeScript, integrating with the backend via a RESTful API  \\n● Enabled real-time hyperparameter tuning (e.g., mutation and survival rates) through an intuitive UI to improve algorithm performance  \\nScribo – AI Email classifier app   github.com/kakolla/Scribo \\nTechnologies: Python, Tkinter, Gmail API, Supervised machine learning, Gemini')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'chunk_id': 5, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 978, 'text_preview': 'Scribo – AI Email classifier app   github.com/kakolla/Scribo \\nTechnologies: Python, Tkinter, Gmail A...'}, page_content='Scribo – AI Email classifier app   github.com/kakolla/Scribo \\nTechnologies: Python, Tkinter, Gmail API, Supervised machine learning, Gemini  \\n● Created a lightweight email-viewing app that classifies and stars unread emails by their importance and displays a to -do summary \\n● Implemented create, update, read, and delete operations using a CRUD architecture  \\n● Designed the Gaussian Naive Bayes classifier from scratch (no Scikit -learn) by programming the matrix multiplication algorithms, \\nperforming feature engineering using self-selected parameters, and training it using 2 months’ worth of supervised email data  \\n \\nSKILLS \\nLanguages: C++, C, Python, C#, Java, Lua, TypeScript, JavaScript, SQL, Cypher, HTML/CSS  \\nFrameworks/Libraries: React, Flask, Node.js, Tailwind, NumPy, TensorFlow, Keras, Scikit-learn, Pandas \\nTools: Git, Docker, Kubernetes, VS Code, Eclipse, Linux/Unix (Debian) \\nTechnologies: Google Gemini, Mistral, Llama 3, MongoDB, Neo4J, LangChain, Pinecone')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'chunk_id': 6, 'source': 'resume.pdf', 'page_number': 0, 'total_length': 194, 'text_preview': 'Tools: Git, Docker, Kubernetes, VS Code, Eclipse, Linux/Unix (Debian) \\nTechnologies: Google Gemini, ...'}, page_content='Tools: Git, Docker, Kubernetes, VS Code, Eclipse, Linux/Unix (Debian) \\nTechnologies: Google Gemini, Mistral, Llama 3, MongoDB, Neo4J, LangChain, Pinecone  \\nOther: Arduino (ATmega328), Solidworks'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Convert processed chunks to Langchain Document for Neo4j db\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=chunk['text'],\n",
    "        metadata=chunk['metadata']\n",
    "    )\n",
    "    for chunk in processed_chunks\n",
    "]\n",
    "\n",
    "# Convert docs to graph format\n",
    "graph_docs = llm_transformer_filtered.convert_to_graph_documents(docs)\n",
    "print(graph_docs)\n",
    "\n",
    "# Add to neo4j\n",
    "graph.add_graph_documents(graph_docs, include_source=True, baseEntityLabel=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
