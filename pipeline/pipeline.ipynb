{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet langchain-neo4j langchain_ollama pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "neo_pass = os.getenv(\"NEO4J_PASS\")\n",
    "\n",
    "url = \"neo4j+s://f5c81351.databases.neo4j.io\"\n",
    "username = \"neo4j\"\n",
    "password = neo_pass\n",
    "graph = Neo4jGraph(url=url, username=username, password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF Documents\n",
    "split text into chunks and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from collections import Counter\n",
    "import re #Regex\n",
    "\n",
    "def _extract_keywords(text: str, top_n: int = 5) -> List[str]:\n",
    "\n",
    "    words = re.findall(r\"r\\w+\", text.lower())\n",
    "\n",
    "    stop_words = set(\n",
    "        [\n",
    "            \"the\",\n",
    "            \"a\",\n",
    "            \"an\",\n",
    "            \"and\",\n",
    "            \"or\",\n",
    "            \"but\",\n",
    "            \"in\",\n",
    "            \"on\",\n",
    "            \"at\",\n",
    "            \"to\",\n",
    "            \"for\",\n",
    "            \"of\",\n",
    "            \"with\",\n",
    "            \"by\",\n",
    "        ]\n",
    "    )\n",
    "    filtered_words = [\n",
    "        word for word in words if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "\n",
    "    return [word for word, count in Counter(filtered_words).most_common(top_n)]\n",
    "\n",
    "def load_and_process_pdf(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict]:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "    pages = loader.load() # load pages\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "\n",
    "    splits = text_splitter.split_documents(pages) # split the pages using LangChain's text_splitter\n",
    "\n",
    "    processed_chunks = []\n",
    "    for i, chunk in enumerate(splits):\n",
    "       metadata = {\n",
    "            \"chunk_id\": i,\n",
    "            \"source\": pdf_path,\n",
    "            \"page_number\": chunk.metadata.get(\"page\", None),\n",
    "            \"total_length\": len(chunk.page_content),\n",
    "            \"keywords\": _extract_keywords(chunk.page_content),\n",
    "            \"text_preview\": (\n",
    "                chunk.page_content[:100] + \"...\"\n",
    "                if len(chunk.page_content) > 100\n",
    "                else chunk.page_content\n",
    "            ),\n",
    "        }\n",
    "       processed_chunks.append({\"text\": chunk.page_content, \"metadata\": metadata})\n",
    "    return processed_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 12\n",
      "\n",
      "Chunk 0:\n",
      "Text Preview: AbhishekKakolla 20243Fri-2PM\n",
      "EvaluatingtheEﬃciencyofSugarcaneandCornFlourasBiofuelsasAlternativestoF...\n",
      "Keywords: ['rcaneandcornflourasbiofuelsasalternativestofossilfuels', 'roduction', 'rameforcompletefossilfueldepletionprojectedtobejustwithinthe', 'rﬁndingalternativestofossilfuelshas', 'remendouslyimportant']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 1:\n",
      "Text Preview: thereisanabsenceofoxygen.Theorganismconvertsacarbohydrate(suchascornorsugarcane)into\n",
      "alcoholoranacid...\n",
      "Keywords: ['rnorsugarcane', 'reisanabsenceofoxygen', 'rganismconvertsacarbohydrate', 'ranacid', 'romycescerevisiae']\n",
      "Page Number: 0\n",
      "\n",
      "Chunk 2:\n",
      "Text Preview: Thus,thisstudywillinvestigatetheeﬃcienciesofusingcornversussugarcaneasorganicsources\n",
      "forethanolferme...\n",
      "Keywords: ['rnversussugarcaneasorganicsources', 'rethanolfermentation', 'redictedthatsugarcanewillbemetabolizedintoethanolfasterthancorn', 'roducethemostcarbondioxidesincecornisknowntobeindigestiblebymanyorganisms', 'rdercarbohydratetobreakdown']\n",
      "Page Number: 1\n",
      "\n",
      "Chunk 3:\n",
      "Text Preview: 3. 4beakerswerepreparedwith1.5gramsofeachreactantcondition.Glucosewasusedasthe\n",
      "positivecontrolasitis...\n",
      "Keywords: ['rol', 'rswerepreparedwith1', 'ramsofeachreactantcondition', 'rolasitisaknowncarbohydratethatbecomesmetabolizedintoethanol', 'rolasitdoesnot']\n",
      "Page Number: 1\n",
      "\n",
      "Chunk 4:\n",
      "Text Preview: 7. Thiswasrepeated3othertimesbyothergroupsforatotalof4trialsforeachcondition.\n",
      "8. TherateandvolumeofC...\n",
      "Keywords: ['riable', 'repeated3othertimesbyothergroupsforatotalof4trialsforeachcondition', 'rateandvolumeofco2', 'roduction', 'remeasuredovertime']\n",
      "Page Number: 2\n",
      "\n",
      "Chunk 5:\n",
      "Text Preview: Figure2:AveragerateofCO2\n",
      "productioninmLforeachconditionattheendofthe21minutes\n",
      "Theaveragesandstandard...\n",
      "Keywords: ['rol', 're2', 'ragerateofco2', 'roductioninmlforeachconditionattheendofthe21minutes', 'ragesandstandarddeviationsofalltrialsforeachconditionwerecomputedintable1']\n",
      "Page Number: 3\n",
      "\n",
      "Chunk 6:\n",
      "Text Preview: Theresultshereareinteresting.First,itsupportsthehypothesisthatsugarcaneproducesthemost\n",
      "CO2\n",
      "andatthef...\n",
      "Keywords: ['resultshereareinteresting', 'rst', 'rtsthehypothesisthatsugarcaneproducesthemost', 'ratesincetherateofco2', 'roductionwasgreaterinthesugarcaneconditionthanin']\n",
      "Page Number: 4\n",
      "\n",
      "Chunk 7:\n",
      "Text Preview: Accordingtotheliterature,juicescontainingfreesugarssuchasfructose,sucrose,andglucose,\n",
      "provetobemoree...\n",
      "Keywords: ['rdingtotheliterature', 'reesugarssuchasfructose', 'rose', 'rovetobemoreeﬃcientinproducingethanolcomparedtostarchysourceslikegrains', 'reesugarsolutionscanbeusedinfermentationwithoutprior']\n",
      "Page Number: 4\n",
      "\n",
      "Chunk 8:\n",
      "Text Preview: andoverallamountofproductionofcarbondioxide(whichistiedtoahigherethanolproduction).Firms\n",
      "andscientis...\n",
      "Keywords: ['rallamountofproductionofcarbondioxide', 'rethanolproduction', 'rms', 'rationwhenconsideringwhichbiofueltousetomaximize', 'rswitchingoutoffossilfuelsandminimizingenvironmentalharm']\n",
      "Page Number: 4\n",
      "\n",
      "Chunk 9:\n",
      "Text Preview: source.Otherstudieshaveshownthatwhilesugarcaneisaneﬃcientsourceforethanolfermentation,it\n",
      "maynotbeeno...\n",
      "Keywords: ['rce', 'rstudieshaveshownthatwhilesugarcaneisaneﬃcientsourceforethanolfermentation', 'replacefossilfuels', 'reexperimentsofthisareaofstudycanincludeinvestigatingtheeﬀectofthetypeofyeastor', 'riaonethanolfermentationasitisunclearatthismomentwhethersaccharomycescerevisiaeisthe']\n",
      "Page Number: 5\n",
      "\n",
      "Chunk 10:\n",
      "Text Preview: References\n",
      "Bai,F.,Anderson,W.,&Moo-Young,M.(2007).Ethanolfermentationtechnologiesfromsugarand\n",
      "starch...\n",
      "Keywords: ['rel', 'riﬁcation', 'references', 'rson', 'rmentationtechnologiesfromsugarand']\n",
      "Page Number: 6\n",
      "\n",
      "Chunk 11:\n",
      "Text Preview: 1–15.https://doi.org/10.1155/2012/989572\n",
      "Maicas,S.(2020).Theroleofyeastsinfermentationprocesses.Micr...\n",
      "Keywords: ['roleofyeastsinfermentationprocesses', 'roorganisms', 'roorganisms8081142', 'rowncropsintheworld2021', 'roduced']\n",
      "Page Number: 6\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"paper.pdf\"\n",
    "\n",
    "chunks = load_and_process_pdf(pdf_path)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"Text Preview: {chunk['metadata']['text_preview']}\")\n",
    "    print(f\"Keywords: {chunk['metadata']['keywords']}\")\n",
    "    print(f\"Page Number: {chunk['metadata']['page_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store data chunks to Neo4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_chunks(chunks: List[Dict]):\n",
    "    # graph.query(\"MATCH (n) DETACH DELETE n\") # cleans graph\n",
    "\n",
    "    # cypher query to create the chunks & their attributes\n",
    "    create_chunk_query = \"\"\"\n",
    "    MERGE (chunk:Chunk {chunk_id: $chunk_id})\n",
    "    ON CREATE SET\n",
    "        chunk.source = $source,\n",
    "        chunk.page_number = $page_number,\n",
    "        chunk.total_length = $total_length,\n",
    "        chunk.text_preview = $text_preview,\n",
    "        chunk.full_text = $full_text\n",
    "        WITH chunk\n",
    "        UNWIND $keywords AS keyword\n",
    "        MERGE (kw:Keyword {name: keyword})\n",
    "        MERGE (chunk)-[:HAS_KEYWORD]->(kw)\n",
    "        RETURN chunk\n",
    "    \"\"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        graph.query(\n",
    "            create_chunk_query,\n",
    "            params={\n",
    "                \"chunk_id\": chunk[\"metadata\"][\"chunk_id\"],\n",
    "                \"source\": chunk[\"metadata\"][\"source\"],\n",
    "                \"page_number\": chunk[\"metadata\"][\"page_number\"],\n",
    "                \"total_length\": chunk[\"metadata\"][\"total_length\"],\n",
    "                \"text_preview\": chunk[\"metadata\"][\"text_preview\"],\n",
    "                \"full_text\": chunk[\"text\"],\n",
    "                \"keywords\": chunk[\"metadata\"][\"keywords\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "create_graph_from_chunks(chunks[:200])\n",
    "\n",
    "# after storing the data, create a unique constraint to make sure data is secure\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunk_id IS UNIQUE\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "embedding_dim = 3072\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector index for similarity search using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.index.vector.createNodeIndex' has been replaced by 'CREATE VECTOR INDEX')} {position: line: 2, column: 13, offset: 13} for query: \"\\n            CALL db.index.vector.createNodeIndex(\\n                'chunk_vector_index',\\n                'Chunk',\\n                'embedding',\\n                $dim,\\n                'cosine'\\n            )\\n            \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/12 chunks\n",
      "Processed 12/12 chunks\n"
     ]
    }
   ],
   "source": [
    "def generate_embedding(text: str) -> List[float]:\n",
    "   \n",
    "    try:\n",
    "        embedding = embeddings.embed_query(text)\n",
    "\n",
    "        embedding = [float(x) for x in embedding]\n",
    "\n",
    "        magnitude = sum(x * x for x in embedding) ** 0.5\n",
    "        if magnitude > 0:\n",
    "            embedding = [x / magnitude for x in embedding]\n",
    "\n",
    "        if len(embedding) != embedding_dim:\n",
    "            if len(embedding) < embedding_dim:\n",
    "                embedding.extend([0.0] * (embedding_dim - len(embedding)))\n",
    "            else:\n",
    "                embedding = embedding[:embedding_dim]\n",
    "\n",
    "        return embedding\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return [0.0] * embedding_dim\n",
    "\n",
    "\n",
    "# we create the vector index using the above function for generating embeddings\n",
    "def create_vector_index(chunks: List[Dict]):\n",
    "\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            DROP INDEX chunk_vector_index IF EXISTS \n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.createNodeIndex(\n",
    "                'chunk_vector_index',\n",
    "                'Chunk',\n",
    "                'embedding',\n",
    "                $dim,\n",
    "                'cosine'\n",
    "            )\n",
    "            \"\"\",\n",
    "            params={\"dim\": embedding_dim},\n",
    "        )\n",
    "\n",
    "        batch_size = 10\n",
    "        total_processed = 0\n",
    "\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i : i + batch_size]\n",
    "            batch_embeddings = []\n",
    "\n",
    "            for chunk in batch:\n",
    "                embedding = generate_embedding(chunk[\"text\"])\n",
    "                batch_embeddings.append(\n",
    "                    {\"chunk_id\": chunk[\"metadata\"][\"chunk_id\"], \"embedding\": embedding}\n",
    "                )\n",
    "\n",
    "            batch_update_query = \"\"\"\n",
    "            UNWIND $batch AS item\n",
    "            MATCH (chunk:Chunk {chunk_id: item.chunk_id})\n",
    "            SET chunk.embedding = item.embedding\n",
    "            \"\"\"\n",
    "\n",
    "            graph.query(batch_update_query, params={\"batch\": batch_embeddings})\n",
    "\n",
    "            total_processed += len(batch)\n",
    "            print(f\"Processed {total_processed}/{len(chunks)} chunks\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector index: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "try:\n",
    "    create_vector_index(chunks[:200])\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create vector index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform vector search on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'chunk_vector_index', 'type': 'VECTOR', 'labelsOrTypes': ['Chunk'], 'properties': ['embedding'], 'options': {'indexProvider': 'vector-2.0', 'indexConfig': {'vector.hnsw.m': 16, 'vector.hnsw.ef_construction': 100, 'vector.dimensions': 3072, 'vector.similarity_function': 'COSINE', 'vector.quantization.enabled': True}}}]\n",
      "{'chunk_id': 16, 'source': 'bisc.pdf', 'page_number': 6, 'text_preview': 'Cleanup and Waste Disposal Instructions   \\n   \\n• Pour all non-hazardous liquid waste (milk and enzym...', 'full_text': 'Cleanup and Waste Disposal Instructions   \\n   \\n• Pour all non-hazardous liquid waste (milk and enzyme solutions) down the sink.   \\n  \\n• Remove all sharpie marks from test tubes and beakers with ethanol and paper towels.   \\n• Rinse, dry and leave them upside-down on you bench for the next lab section.   \\n  \\n• Discard used glucose strips and the blue pipette tips in the biohazard bin.   \\n  \\n• Wipe down the bench with Lysol and paper towels.', 'total_length': 442, 'score': 0.7241970300674438}\n",
      "16\n",
      "bisc.pdf\n",
      "6\n",
      "Cleanup and Waste Disposal Instructions   \n",
      "   \n",
      "• Pour all non-hazardous liquid waste (milk and enzym...\n",
      "{'chunk_id': 1, 'source': 'Resume_Abi_Kakolla.pdf', 'page_number': 0, 'text_preview': '● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural ...', 'full_text': '● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural network using MeshLab and Python  \\n● Developed open-source software to grid a dataset of 6 million neural points that enhanced the neural mesh model resolution by 348%, \\nenabling other teams to create more accurate models and save on computational costs  \\n● Reduced neural network generation by 75% by integrating unsupervised machine learning algorithms, enabling faster simulations  \\n● Currently developing a Hybrid Retrieval-Augmented Generation (RAG) pipeline for research faculty integrating Graph retrieval agents \\nwith Neo4j and LangChain, and vector-based agents with Pinecone, to enable neuroscience-focused text and citation generation \\nLead Software Developer                  Nov 2021 - June 2022 \\ninLoop (sponsored by Deloitte)                                                                           Toronto, ON', 'total_length': 926, 'score': 0.7207645773887634}\n",
      "1\n",
      "Resume_Abi_Kakolla.pdf\n",
      "0\n",
      "● Modeled in-silico layers of the Hippocampus used to generate a dendritic tree as part of a neural ...\n"
     ]
    }
   ],
   "source": [
    "def verify_vector_index():\n",
    "    query = \"\"\"\n",
    "    SHOW INDEXES\n",
    "    YIELD name, type, labelsOrTypes, properties, options\n",
    "    WHERE name = 'chunk_vector_index'\n",
    "    \"\"\"\n",
    "    return graph.query(query)\n",
    "\n",
    "\n",
    "def vector_search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "   \n",
    "    try:\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "        search_query = \"\"\"\n",
    "        MATCH (c:Chunk)\n",
    "        WITH c, vector.similarity.cosine(c.embedding, $embedding) AS score\n",
    "        WHERE score > 0.7\n",
    "        RETURN \n",
    "            c.chunk_id AS chunk_id,\n",
    "            c.source AS source,\n",
    "            c.page_number AS page_number,\n",
    "            c.text_preview AS text_preview,\n",
    "            c.full_text AS full_text,\n",
    "            c.total_length AS total_length,\n",
    "            score\n",
    "        ORDER BY score DESC\n",
    "        LIMIT $limit\n",
    "        \"\"\"\n",
    "\n",
    "        results = graph.query(\n",
    "            search_query, params={\"embedding\": query_embedding, \"limit\": top_k}\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Vector search error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(verify_vector_index())\n",
    "for x in vector_search(\"What is a biofuel?\"):\n",
    "    print(x)\n",
    "    print(x['chunk_id'])\n",
    "    print(x['source'])\n",
    "    print(x['page_number'])\n",
    "    print(x['text_preview'])\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "\n",
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,  \n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='chunk_vector_index',  \n",
    "    node_label='Chunk',  \n",
    "    text_node_properties=['full_text'], \n",
    "    embedding_node_property='embedding'\n",
    ")\n",
    "\n",
    "retriever = neo4j_vector_store.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Initialize the Ollama model\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What were the disposal instructions in the lab report?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
